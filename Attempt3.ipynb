{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import codecs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import bigrams \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from nltk import PorterStemmer\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeTAGS(x):\n",
    "    x = re.sub(r'\\s+',r' ',x)\n",
    "    x = re.sub('<code>.*?code>', r' ', x)\n",
    "    x = re.sub('<.*?>', r' ', x)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeUrls(x):\n",
    "    x = x.lower()\n",
    "    x = re.sub('http.*?\\s', r' ', x)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removePunctuation(x):\n",
    "    x = x.lower()\n",
    "    x = re.sub(r'[^\\x00-\\x7f]',r' ',x) #For Characters like Ã‚\n",
    "    x = re.sub(r'\\s+',r' ',x) #Multiple Spaces, tabs, carriage returns to single space\n",
    "    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeNos(x):\n",
    "    x = re.sub(r'[^a-z\\s]', r' ', x)\n",
    "    x = re.sub(r'\\s+',r' ',x)\n",
    "    x = re.sub(r'(\\s\\w{1,2})+\\s', r' ',x)\n",
    "    x = re.sub(r'\\s+',r' ',x)\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "stops.update([\"would\", \"get\", \"like\", \"likely\", \"using\", \"know\", \"question\", \"use\", \"get\", \"possible\" , \"much\", \"find\", \"anyone\"])\n",
    "stops.update([\"tell\", \"know\", \"another\", \"various\", \"also\", \"etc\", \"around\", \"vs\", \"used\", \"could\", \"without\", \"way\", \"new\"])\n",
    "stops.update([\"need\", \"known\", \"make\", \"makes\", \"made\", \"file\", \"downloaded\", \"download\", \"one\", \"two\", \"even\", \"far\", \"thus\"])\n",
    "stops.update([\"every\", \"said\", \"call\", \"tell\", \"tells\", \"called\", \"per\", \"though\", \"less\",\"since\", \"problem\", \"answer\", \"answers\"])\n",
    "stops.update([\"small\", \"vast\",\"therefore\", \"must\", \"simply\", \"put\", \"less\", \"true\", \"talk\", \"said\", \"per\", \"related\"])\n",
    "stops.update([\"need\", \"needed\", \"variety\", \"cause\", \"causes\", \"later\", \"unrelated\", \"entire\", \"saying\", \"give\", \"gave\", \"false\"])         \n",
    "stops.update([\"show\", \"shows\", \"however\", \"previous\", \"previously\", \"average\", \"mean\", \"think\", \"thought\", \"little\", \"according\"])\n",
    "stops.update([\"number\", \"view\", \"views\", \"viewed\", \"send\", \"sent\", \"may\", \"might\", \"right\", \"wrong\", \"high\", \"low\" ])\n",
    "stops.update([\"min\", \"see\", \"saw\", \"sees\", \"someone\", \"please\", \"explain\", \"explains\", \"text\", \"texts\", \"indicate\", \"indicates\"])              \n",
    "stops.update([\"begin\", \"began\", \"take\", \"taken\", \"array\", \"arrive\", \"conclusion\", \"conclusions\", \"hline\", \"amp\", \"edit\", \"end\"])\n",
    "stops.update([\"thank\", \"thanks\", \"assistance\", \"end\", \"error\", \"left\", \"right\", \"suppose\", \"sample\", \"given\", \"best\", \"guess\"])\n",
    "stops.update([\"figure\", \"matlab\", \"parameters\", \"parameter\",\"paper\", \"papers\",\"whether\", \"understand\",  \"quite\", \"work\", \"work\"]) \n",
    "stops.update([\"clears\", \"obtain\", \"obtains\", \"obtained\", \"form\", \"forms\", \"find\", \"found\", \"example\", \"examples\"])\n",
    "stops.update([\"better\", \"nothing\", \"different\", \"well\", \"say\", \"says\", \"seem\", \"huge\", \"large\", \"definition\", \"meaning\"])\n",
    "stops.update([\"larger\", \"many\", \"seems\", \"pretty\", \"large\", \"never\", \"big\", \"bigger\", \"yet\", \"detail\", \"details\", \"nbsp\"])\n",
    "stops.update([\"cdot\", \"vec\", \"table\", \"thing\", \"either\", \"wait\", \"overview\", \"something\", \"else\", \"came\", \"across\"])\n",
    "stops.update([\"wikipedia\", \"article\", \"feel\", \"free\", \"stack\", \"exchange\", \"quick\", \"google\", \"anything\", \"read\", \"wiki\"])\n",
    "stops.update([\"appreciate\", \"help\", \"got\", \"thin\", \"thinking\", \"scholar\", \"several\", \"times\", \"currently\", \"working\", \"studied\"])\n",
    "stops.update([\"really\", \"want\", \"text\", \"sure\", \"correct\", \"simple\", \"language\", \"wanted\", \"ask\", \"read\", \"lot\", \"takes\"])\n",
    "stops.update([\"let\", \"consider\", \"year\", \"years\", \"ago\", \"long\", \"term\"])\n",
    "#\n",
    "stops.update([\"look\", \"looks\", \"looked\", \"case\", \"cases\", \"wonder\", \"wondering\", \"come\", \"comes\", \"change\", \"changes\", \"changing\" ])\n",
    "stops.update([\"look\",\"changed\", \"differ\", \"effect\", \"follow\", \"function\", \"happen\", \"inform\", \"look\", \"peopl\", \"produc\" ])\n",
    "stops.update([\"process\",\"read\", \"studi\", \"reason\", \"inform\", \"result\", \"similar\", \"time\", \"type\"])\n",
    "stops.update([\"abl\", \"actual\", \"alway\", \"amount\", \"appear\", \"back\", \"becom\", \"book\", \"clear\", \"common\", \"compar\", \"contain\"])\n",
    "stops.update([\"determin\", \"develop\", \"due\", \"contain\", \"exact\", \"explan\", \"express\", \"first\", \"general\", \"good\", \"idea\", \"identifi\"])\n",
    "stops.update([\"includ\", \"increas\", \"interest\", \"kind\", \"learn\", \"lead\", \"link\", \"level\", \"measur\", \"method\", \"name\", \"often\"])\n",
    "stops.update([\"place\", \"point\", \"rather\", \"provid\", \"refer\", \"respons\", \"search\", \"seen\", \"select\", \"requir\", \"relat\", \"recent\"])\n",
    "stops.update([\"start\", \"still\", \"within\", \"certain\", \"complet\", \"research\", \"day\", \"enough\", \"least\", \"direct\", \"test\"])\n",
    "stops.update([\"base\", \"exist\", \"factor\", \"grow\", \"live\", \"mormal\", \"occur\", \"order\",\"suggest\", \"present\", \"part\", \"particular\"])\n",
    "stops.update([\"via\",\"word\", \"whole\", \"valu\", \"usual\", \"turn\", \"total\", \"step\", \"sourc\", \"sort\", \"size\", \"side\", \"short\", \"set\"])\n",
    "stops.update([\"second\", \"run\", \"remov\", \"regard\", \"possibl\", \"perhap\", \"note\", \"move\", \"mayb\", \"main\", \"lower\", \"list\"])\n",
    "stops.update([\"keep\", \"last\", \"involv\", \"insid\", \"import\", \"higher\", \"go\", \"group\", \"generat\", \"fact\", \"expect\", \"done\"])\n",
    "stops.update([\"obvious\",\"origin\",\"popul\", \"pathway\", \"creat\", \"assum\", \"allow\", \"advantag\", \"caus\", \"color\", \"creat\", \"defin\"])\n",
    "stops.update([\"describ\", \"consid\", \"avail\", \"condit\", \"confus\", \"depend\", \"eat\", \"heard\", \"instead\", \"length\", \"locat\", \"multipl\"])\n",
    "stops.update([\"limit\", \"imagin\", \"cours\", \"area\", \"depend\"])\n",
    "stops.update([\"curious\",\"close\", \"believ\", \"alreadi\", \"affect\", \"tri\"])\n",
    "stops.update([\"advanc\", \"bind\", \"bit\", \"calcul\", \"code\", \"mention\", \"observ\", \"tri\", \"theori\", \"rate\", \"non\", \"notic\", ])\n",
    "stops.update([\"individu\", \"infec\", \"normal\", \"pair\", \"perform\", \"physic\", \"pictur\", \"post\", \"probabl\", \"posit\", \"specif\", \"theori\"])\n",
    "stops.update([\"comput\", \"state\", \"analysi\", \"addit\", \"activ\", \"concentr\", \"control\", \"current\", \"cycl\", \"evid\", \"imag\"])\n",
    "stops.update([\"infect\", \"interact\", \"line\", \"singl\", \"signific\", \"surviv\"])\n",
    "stops.update([\"abil\", \"act\", \"action\", \"add\", \"age\", \"almost\", \"along\", \"altern\", \"although\", \"among\", \"appreci\", \"background\"])\n",
    "stops.update([\"appli\", \"associ\", \"basic\", \"behind\", \"carri\", \"complex\", \"concept\", \"continu\", \"cross\", \"cut\", \"damag\"])\n",
    "stops.update([\"decreas\", \"detect\", \"discuss\", \"effic\", \"especi\", \"estim\", \"ever\", \"experience\", \"experiences\", \"extract\"])\n",
    "stops.update([\"format\", \"great\", \"hard\", \"hope\", \"hour\", \"initi\", \"issu\", \"lack\", \"leav\", \"longer\", \"materi\", \"near\", \"next\"])\n",
    "stops.update([\"old\", \"open\", \"outsid\", \"page\", \"pass\", \"prefer\", \"pressur\", \"product\", \"reach\", \"real\", \"red\", \"releas\"])\n",
    "stops.update([\"remain\", \"rest\", \"sampl\", \"separ\", \"site\", \"sometim\", \"stop\", \"strand\", \"strong\", \"subject\", \"three\", \"today\"])\n",
    "stops.update([\"togeth\", \"topic\", \"trait\", \"transcript\", \"typic\", \"white\", \"target\", \"subject\",\"solut\",\"sound\"])\n",
    "stops.update([\"claim\", \"connect\", \"effici\", \"frequenc\", \"instanc\", \"knowledg\", \"lab\", \"negat\", \"pattern\", \"necessari\", \"matter\"])\n",
    "stops.update([\"life\", \"negat\", \"period\", \"purpos\", \"random\", \"rang\",\"reduc\", \"regul\", \"replic\", \"variat\", \"light\", \"kill\"])\n",
    "stops.update([\"prevent\", \"standard\", \"role\", \"signal\"])\n",
    "stops.update([\"account\", \"air\", \"ad\", \"align\", \"angl\", \n",
    "\"angular\", \"appar\", \"applic\", \"approach\",\n",
    "\"approxim\", \"arbitrari\", \"argument\", \"assumpt\",\n",
    "\"attract\", \"away\", \"axi\", \"ball\", \"bar\", \"basi\",\n",
    "\"beam\", \"beta\", \"bodi\", \"boundari\", \n",
    "\"break\", \"center\", \"classically\", \"classicality\",\n",
    "\"coeffici\", \"collis\", \"combin\", \"compon\", \n",
    "\"consist\", \"constant\", \"construct\", \"context\",\n",
    "\"coordin\", \"correspond\", \"cos\", \"curv\", \"definit\",\n",
    "\"degre\", \"delta\", \"densiti\", \"deriv\", \"diagram\",\n",
    "\"dimens\", \"dimension\", \"distribut\", \"dot\", \"drop\", \"easi\", \"emit\", \"ensilon\", \"equal\",\n",
    "\"equat\", \"equival\", \"event\", \"everyth\", \"expand\",\n",
    "\"expans\", \"extern\", \"fall\", \"faster\", \"fielding\",\n",
    "\"fielded\", \"final\", \"finit\", \"fix\", \"flow\", \"frac\", \n",
    "\"formula\", \"frame\", \"full\", \"fundament\",\n",
    "\"gas\", \"gaug\", \"goe\", \"greater\", \"ground\",\n",
    "\"half\", \"hand\", \"hat\", \"height\", \"hence\", \"hit\",\n",
    "\"hold\", \"holed\", \"hoizont\", \"ideal\", \"ident\",\n",
    "\"ignor\", \"impli\", \"independ\", \"infinit\", \"infti\", \n",
    "\"int\", \"integr\", \"intern\", \"interpret\", \"introduc\",\n",
    "\"intuit\", \"invari\", \"lambda\", \"langl\", \"law\", \n",
    "\"linear\", \"local\", \"magnitud\", \"mass\", \"mathbf\",\n",
    "\"mathcal\", \"mathrm\", \"matrix\", \"mathemat\",\n",
    "\"maximum\", \"metric\", \"miss\", \"nabla\", \"natur\",\n",
    "\"net\", \"object\", \"omega\", \"oper\", \"opposit\",\n",
    "\"parallel\", \"partial\", \"path\", \"perfect\", \"perpendicular\", \"phase\", \"phi\", \"plane\",\n",
    "\"potenti\", \"power\", \"precis\", \"predict\", \n",
    "\"principl\", \"project\", \"proof\", \"propag\",\n",
    "\"proper\", \"properti\", \"proport\", \"prove\", \"psi\",\n",
    "\"qft\", \"quantiti\", \"radius\", \"rangl\", \"ray\",\n",
    "\"reflect\", \"regional\", \"relationship\",\n",
    "\"repres\", \"represent\", \"respect\", \"rho\", \n",
    "\"rightarrow\", \"rule\", \"satisfi\", \"scalar\",\n",
    "\"scale\", \"scatter\", \"section\", \"sens\", \n",
    "\"shape\", \"shown\", \"sigma\", \"simul\",\n",
    "\"sin\", \"situat\", \"smaller\", \"solid\", \"solv\",\n",
    "\"somehow\", \"spacing\", \"spaced\", \"spacings\",\n",
    "\"special\", \"sphere\", \"spheric\", \"spin\", \"sqrt\",\n",
    "\"squar\", \"star\", \"statement\", \"stationari\", \n",
    "\"straight\", \"structur\", \"sum\", \"sun\", \"suppos\",\n",
    "\"symmetri\", \"textbook\", \"theoret\", \"theta\", \"top\", \n",
    "\"toward\", \"transfer\", \"transform\", \"travel\", \n",
    "\"troubl\", \"uniform\", \"unit\", \"universal\",\n",
    "\"university\", \"universality\", \"universities\",\n",
    "\"valid\", \"vari\", \"variabl\", \"vector\", \"vertic\",\n",
    "\"volum\", \"water\", \"weight\", \"world\", \"write\", \n",
    "\"written\", \"yes\", \"zero\"\n",
    "])\n",
    "stop_words = pd.DataFrame(list(stops), columns = [\"words\"])\n",
    "stop_words = stop_words.sort_values(['words'])\n",
    "stop_words.to_csv(\"stop_words.csv\", index= False)\n",
    "#stop_words.sort_values(['frequency'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeStopwords(x):\n",
    "    # Removing all the stopwords\n",
    "    filtered_words = [word for word in x.split() if word not in stops]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.08333333333333333, 0.08333333333333333, 0.058823529411764705, 0.0625, 0.058823529411764705, 0.5, 0.043478260869565216]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#similarity2(\"feline\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def similarity(w1, w2):\n",
    "    synsets1 = wn.synsets(w1)\n",
    "    synsets2 = wn.synsets(w2)\n",
    "    sim_scores = []\n",
    "    for synset1 in range(len(synsets1)):\n",
    "        for synset2 in range(len(synsets2)):\n",
    "            if (synsets1[synset1].wup_similarity(synsets2[synset2], simulate_root = False) != None) \\\n",
    "            and synsets1[synset1].wup_similarity(synsets2[synset2], simulate_root = False) >= 0 :\n",
    "                sim_scores.append(synsets1[synset1].wup_similarity(synsets2[synset2], simulate_root = False))\n",
    "    print(sim_scores)\n",
    "    if len(sim_scores) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_synonym(word, word2):\n",
    "    l_syns = list()\n",
    "    synsets = wn.synsets(word)\n",
    "    for synset in synsets:\n",
    "        if word2 in synset.lemma_names():\n",
    "            l_syns.append( (word, word2) )\n",
    "    return l_syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity2(w1, w2):\n",
    "    synsets1 = wn.synsets(w1)\n",
    "    synsets2 = wn.synsets(w2)\n",
    "    sim_scores = []\n",
    "    for synset1 in range(len(synsets1)):\n",
    "        for synset2 in range(len(synsets2)):\n",
    "            if (synsets1[synset1].path_similarity(synsets2[synset2]) != None) \\\n",
    "            and synsets1[synset1].path_similarity(synsets2[synset2]) >= 0.5 :\n",
    "                sim_scores.append(synsets1[synset1].path_similarity(synsets2[synset2]))\n",
    "    #print(sim_scores)\n",
    "    if len(sim_scores) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check_synonym(\"molecule\",\"feline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#syns = wn.synsets(\"program\")\n",
    "#syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cat = wn.synset('feline.n.01')\n",
    "#cat.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#synsets1 = wn.synsets(\"feline\")\n",
    "#synsets1\n",
    "#synsets1[2].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#synsets2 = wn.synsets(\"evolutionary\")\n",
    "#synsets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(synsets1[1].wup_similarity(synsets2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #similarity(\"genetic\", \"genome\")\n",
    "# synsets1 = wn.synsets(\"cat\")\n",
    "# print(synsets1)\n",
    "# synsets2 = wn.synsets(\"feline\")\n",
    "# print(synsets2)\n",
    "# type(synsets1)\n",
    "\n",
    "# if synsets1[0] == synsets2[0]:\n",
    "#     print(\"True\")\n",
    "# else:\n",
    "#     print(\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.wordnet.Synset"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(synsets1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.wordnet.Synset"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nerve_cell.n.01 ['nerve_cell', 'neuron']\n"
     ]
    }
   ],
   "source": [
    "# for ss in wn.synsets('neuron'):\n",
    "#     print(ss.name(), ss.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nerve_cell', 'neuron'}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from itertools import chain\n",
    "# synonyms = wn.synsets('neuron')\n",
    "# set(chain.from_iterable([word.lemma_names() for word in synonyms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# w1 = wn.synset('gene.n.01')\n",
    "# w2 = wn.synset('genetic.n.01')\n",
    "# print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biology = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biology['combine'] = \" \" + biology['title'] + \" \" + biology['content'] + \" \"\n",
    "biology['combiny'] = biology['combine'].map(removeTAGS)\n",
    "biology['combiny'] = biology['combiny'].map(removeUrls)\n",
    "biology['combiny'] = biology['combiny'].map(removePunctuation)\n",
    "biology['combiny'] = biology['combiny'].map(removeNos)\n",
    "biology['combiny'] = biology['combiny'].map(removeStopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# biology[\"tag_list\"] = biology[\"tags\"].map(lambda x: x.split())\n",
    "# tags = biology[\"tag_list\"].tolist()\n",
    "# all_tags = [j for i in range(len(tags)) for j in tags[i]]\n",
    "# count = pd.DataFrame(all_tags, columns=[\"word\"])\n",
    "# count1 = count.groupby([\"word\"]).size().reset_index(name='count')\n",
    "# count1.sort_values(by=[\"count\"], inplace = True, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "combiny = biology['combiny'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokens = [token for sent in nltk.sent_tokenize(text) for token in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if re.search(r'[a-zA-Z]',token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]      \n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [ token for sent in nltk.sent_tokenize(text) for token in nltk.word_tokenize(sent)]\n",
    "    filtered_token = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if re.search(r'[a-zA-Z]', token):\n",
    "            filtered_token.append(token)\n",
    "    return filtered_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_text=[]\n",
    "stemmed_text = []\n",
    "for i in combiny:\n",
    "    stemmed_text.extend(tokenize_and_stem(i))\n",
    "    filtered_text.extend(tokenize_only(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#len(stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3330665"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56816"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(filtered_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stem</th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16068</th>\n",
       "      <td>energi</td>\n",
       "      <td>energy</td>\n",
       "      <td>38139</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18313</th>\n",
       "      <td>field</td>\n",
       "      <td>field</td>\n",
       "      <td>27987</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16468</th>\n",
       "      <td>equat</td>\n",
       "      <td>equation</td>\n",
       "      <td>22295</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19066</th>\n",
       "      <td>forc</td>\n",
       "      <td>force</td>\n",
       "      <td>22156</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         stem      word  count  length\n",
       "16068  energi    energy  38139       6\n",
       "18313   field     field  27987       5\n",
       "16468   equat  equation  22295       8\n",
       "19066    forc     force  22156       5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_dict = pd.DataFrame({'stem':stemmed_text, 'word':filtered_text})\n",
    "stemmed_dict_freq = stemmed_dict.groupby([\"stem\",\"word\"]).size().reset_index(name='count')\n",
    "stemmed_dict_freq[\"length\"] = stemmed_dict_freq[\"word\"].apply(len)\n",
    "stemmed_dict_freq.sort_values(by = [\"count\",\"length\"], inplace= True, ascending = [False, False] )\n",
    "stemmed_dict_freq.iloc[0:4,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = stemmed_dict_freq.set_index([\"stem\"])\n",
    "def fa(x):\n",
    "    if type(x) == type(\"word\") :\n",
    "        return([x])\n",
    "    else:\n",
    "        return x.tolist()\n",
    "k_1 = {w : fa(k.ix[w,0]) for w in set(stemmed_text)}\n",
    "#k_1[\"tri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup = stemmed_dict_freq.groupby([\"stem\"]).first().reset_index()\n",
    "lookup2 = lookup.set_index([\"stem\"])\n",
    "#lookup2.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 59s\n",
      "(81926, 93)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.015, stop_words=stops,\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(0,1))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(combiny) #fit the vectorizer to content\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceler ['acceleration', 'accelerating', 'accelerate', 'accelerated', 'accelerates', 'accelerations', 'accelerator', 'accelerators', 'accelerational', 'acceleratingly']\n",
      "alpha ['alpha', 'alphas']\n",
      "amplitud ['amplitude', 'amplitudes', 'amplitud', 'amplituded']\n",
      "atom ['atom', 'atoms', 'atomic', 'atomically', 'atomizers', 'atomizer', 'atomical', 'atomized', 'atomism']\n",
      "attempt ['attempt', 'attempting', 'attempts', 'attempted']\n",
      "black ['black', 'blackness', 'blacking']\n",
      "boson ['boson', 'bosons', 'bosonic', 'bosonization', 'bosonized', 'bosonize', 'bosonizing']\n",
      "charg ['charge', 'charges', 'charged', 'charging', 'chargs', 'charg']\n",
      "circuit ['circuit', 'circuits', 'circuital', 'circuited', 'circuitous', 'circuitation']\n",
      "circular ['circular', 'circularly', 'circularity', 'circularization', 'circularize']\n",
      "classic ['classical', 'classic', 'classics', 'classicalness']\n",
      "conduct ['conducting', 'conduction', 'conductivity', 'conductive', 'conduct', 'conducted', 'conductance', 'conducts', 'conductivities', 'conductances', 'conductiveness', 'conductively', 'conductible', 'conducter']\n",
      "conserv ['conservation', 'conserved', 'conservative', 'conserve', 'conserving', 'conserves', 'conservations', 'conservatively', 'conservativity', 'conservance', 'conserv']\n",
      "coupl ['coupling', 'couple', 'coupled', 'couplings', 'couples']\n",
      "data ['data', 'datas']\n",
      "decay ['decay', 'decays', 'decaying', 'decayed']\n",
      "differenti ['differential', 'differentiate', 'differentiation', 'differentiating', 'differentiable', 'differentials', 'differentiated', 'differentiates', 'differentiability', 'differentially', 'differentiabilities', 'differentiations', 'differentiably']\n",
      "distanc ['distance', 'distances', 'distanced', 'distancing']\n",
      "dynam ['dynamics', 'dynamical', 'dynamic', 'dynamically']\n",
      "earth ['earth', 'earths', 'earthed', 'earthing', 'earthly']\n",
      "einstein ['einstein', 'einsteins']\n",
      "electr ['electric', 'electrical', 'electricity', 'electrically', 'electrics', 'electricly', 'electricals', 'electr']\n",
      "electromagnet ['electromagnetic', 'electromagnetism', 'electromagnet', 'electromagnets', 'electromagnetics', 'electromagnetically', 'electromagnetical']\n",
      "electron ['electron', 'electrons', 'electronic', 'electronics', 'electronically', 'electrones', 'electrone']\n",
      "element ['elements', 'element', 'elemental']\n",
      "energi ['energy', 'energies', 'energie', 'energys']\n",
      "engin ['engine', 'engineering', 'engines', 'engineer', 'engineers', 'engineered', 'enginering', 'engin']\n",
      "entropi ['entropy', 'entropies', 'entropie']\n",
      "epsilon ['epsilon', 'epsilons']\n",
      "equilibrium ['equilibrium', 'equilibriums']\n",
      "experi ['experiment', 'experiments', 'experimenting', 'experimented', 'experiement', 'experiements', 'experimently', 'experient']\n",
      "experiment ['experimental', 'experimentally', 'experimenter', 'experimenters', 'experimentation', 'experimentations']\n",
      "field ['field', 'fields']\n",
      "fluid ['fluid', 'fluids', 'fluide', 'fluides', 'fluidly']\n",
      "forc ['force', 'forces', 'forced', 'forcing', 'forcefully', 'forceful', 'forcings', 'forc']\n",
      "friction ['friction', 'frictional', 'frictions']\n",
      "gamma ['gamma', 'gammas']\n",
      "gravit ['gravitational', 'gravitation', 'gravitationally', 'gravitional', 'gravitate', 'gravitating', 'gravitates', 'gravitator', 'gravition', 'gravit', 'gravitated', 'gravitions', 'gravitationals', 'graviting', 'gravitic']\n",
      "graviti ['gravity', 'gravities', 'gravitiational', 'gravitiation', 'gravitys', 'graviti']\n",
      "hamiltonian ['hamiltonian', 'hamiltonians']\n",
      "hbar ['hbar']\n",
      "heat ['heat', 'heating', 'heated', 'heats', 'heatings', 'heates']\n",
      "hole ['hole', 'holes']\n",
      "horizont ['horizontal', 'horizontally', 'horizont', 'horizontals']\n",
      "kinet ['kinetic', 'kinetics', 'kinetically', 'kinetical']\n",
      "lagrangian ['lagrangian', 'lagrangians']\n",
      "liquid ['liquid', 'liquids', 'liquidators', 'liquidity', 'liquide']\n",
      "lorentz ['lorentz']\n",
      "magnet ['magnetic', 'magnet', 'magnets', 'magnetism', 'magnetization', 'magnetized', 'magnetically', 'magnetize', 'magnetizing', 'magnetics', 'magnetical', 'magnetizations']\n",
      "math ['math', 'maths']\n",
      "mechan ['mechanics', 'mechanical', 'mechanism', 'mechanisms', 'mechanically', 'mechanic']\n",
      "metal ['metal', 'metals', 'metallic', 'metallicity', 'metalic', 'metallicities', 'metaled']\n",
      "model ['model', 'models', 'modeling', 'modeled', 'modelling', 'modelled', 'modell', 'modelize', 'modelization', 'modelizations', 'modellization', 'modelable', 'modelings', 'modelized', 'modells']\n",
      "molecul ['molecules', 'molecule', 'moleculer', 'moleculs']\n",
      "moment ['moment', 'moments']\n",
      "momentum ['momentum', 'momentums']\n",
      "motion ['motion', 'motions', 'motional']\n",
      "newton ['newton', 'newtons']\n",
      "optic ['optical', 'optics', 'optic', 'optically']\n",
      "orbit ['orbit', 'orbital', 'orbits', 'orbiting', 'orbitals', 'orbited', 'orbiter', 'orbitting', 'orbiters', 'orbites', 'orbite']\n",
      "oscil ['oscillator', 'oscillation', 'oscillations', 'oscillating', 'oscillators', 'oscillate', 'oscillates', 'oscilator', 'oscilating', 'oscillated', 'oscilation', 'oscilators', 'oscilations', 'oscilate', 'oscilates']\n",
      "particl ['particle', 'particles']\n",
      "photon ['photon', 'photons', 'photonic', 'photonics']\n",
      "physicist ['physicists', 'physicist']\n",
      "planet ['planet', 'planets']\n",
      "polar ['polarization', 'polarized', 'polar', 'polarizer', 'polarity', 'polarizations', 'polarizing', 'polarizers', 'polarize', 'polarities', 'polarizes', 'polars']\n",
      "proton ['proton', 'protons', 'protonation', 'protonic']\n",
      "pull ['pull', 'pulling', 'pulled', 'pulls']\n",
      "pure ['pure', 'purely']\n",
      "quantum ['quantum', 'quantumly', 'quantumness', 'quantume', 'quantums']\n",
      "radiat ['radiation', 'radiate', 'radiative', 'radiated', 'radiating', 'radiates', 'radiations', 'radiator', 'radiators', 'radiatively', 'radiational']\n",
      "region ['region', 'regions']\n",
      "relativist ['relativistic', 'relativistically', 'relativist', 'relativisticly', 'relativists', 'relativistical', 'relativistics']\n",
      "resist ['resistance', 'resistivity', 'resistive', 'resistances', 'resist', 'resistant', 'resisting', 'resists', 'resistence', 'resisted', 'resistivities', 'resister', 'resisters', 'resistively', 'resistency', 'resiste']\n",
      "rotat ['rotation', 'rotating', 'rotational', 'rotate', 'rotates', 'rotations', 'rotated', 'rotator', 'rotatable', 'rotatating', 'rotatation', 'rotatates', 'rotators']\n",
      "space ['space', 'spaces']\n",
      "spacetim ['spacetime', 'spacetimes']\n",
      "speed ['speed', 'speeds', 'speeding', 'speeded']\n",
      "string ['string', 'strings', 'stringed', 'stringing', 'stringly']\n",
      "surfac ['surface', 'surfaces', 'surfaced', 'surfacing', 'surfac']\n",
      "system ['system', 'systems', 'systemically', 'systemic', 'systeme', 'systemes']\n",
      "temperatur ['temperature', 'temperatures', 'temperatured', 'temperaturs', 'temperatur']\n",
      "tensor ['tensor', 'tensors', 'tensored', 'tensoring']\n",
      "theorem ['theorem', 'theorems']\n",
      "thermal ['thermal', 'thermally', 'thermalization', 'thermalize', 'thermalized', 'thermalizes', 'thermalizing', 'thermals']\n",
      "thermodynam ['thermodynamics', 'thermodynamic', 'thermodynamical', 'thermodynamically']\n",
      "univers ['universe', 'universes', 'universally']\n",
      "vacuum ['vacuum', 'vacuums', 'vacuumed', 'vacuuming', 'vacuume', 'vacuumized']\n",
      "veloc ['velocity', 'velocities', 'velocitys']\n",
      "voltag ['voltage', 'voltages']\n",
      "wave ['wave', 'waves', 'waving', 'waved']\n",
      "wavelength ['wavelength', 'wavelengths', 'wavelengthness']\n",
      "wire ['wire', 'wires', 'wired', 'wiring']\n"
     ]
    }
   ],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "type(terms)\n",
    "terms\n",
    "#terms.apply(lambda x: lookup2.ix[terms[x]]['word'])\n",
    "words = [lookup2.ix[terms[x]]['word'] for x in range(len(terms))]\n",
    "words\n",
    "\n",
    "for a in terms:\n",
    "    print(a, k_1[a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word_index = tfidf_matrix[:,terms.index(\"allel\")].nonzero()[0].tolist()\n",
    "#list(combiny[i] for i in word_index[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dst = 1-cosine_similarity(tfidf_matrix)\n",
    "#type(dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#corr = pd.DataFrame(dst)\n",
    "#corr.to_csv(\"corr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#count1.iloc[0:20,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['doc_cluster.pkl', 'doc_cluster.pkl_01.npy', 'doc_cluster.pkl_02.npy']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 100;\n",
    "km = KMeans(n_clusters = num_clusters)\n",
    "%time km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()\n",
    "joblib.dump(km,  'doc_cluster.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "order_centroids = km.cluster_centers_.argsort()[:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 91)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#order_centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = pd.Series(clusters)\n",
    "biology[\"cluster\"] = m.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_words(x):\n",
    "    ind = order_centroids[x,:6].tolist()\n",
    "    list = []\n",
    "    for inw in ind:\n",
    "        list.append(lookup2.ix[terms[inw]]['word'])\n",
    "    return list\n",
    "\n",
    "def top_stemmed(x):\n",
    "    ind = order_centroids[x,:6].tolist()\n",
    "    return [terms[inw] for inw in ind]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['species', 'human', 'biology', 'evolution', 'organism', 'environment']\n",
      "['speci', 'human', 'biolog', 'evolut', 'organ', 'environ']\n",
      "1\n",
      "['immune', 'system', 'cells', 'body', 'virus', 'disease']\n",
      "['immun', 'system', 'cell', 'bodi', 'virus', 'diseas']\n",
      "2\n",
      "['human', 'species', 'biology', 'evolve', 'sense', 'evolution']\n",
      "['human', 'speci', 'biolog', 'evolv', 'sens', 'evolut']\n",
      "3\n",
      "['brain', 'biology', 'person', 'human', 'cells', 'physiological']\n",
      "['brain', 'biolog', 'person', 'human', 'cell', 'physiolog']\n",
      "4\n",
      "['gene', 'sequence', 'genome', 'protein', 'dna', 'rna']\n",
      "['gene', 'sequenc', 'genom', 'protein', 'dna', 'rna']\n",
      "5\n",
      "['sense', 'field', 'environment', 'adaptation', 'molecular', 'parent']\n",
      "['sens', 'field', 'environ', 'adapt', 'molecular', 'parent']\n",
      "6\n",
      "['dna', 'cells', 'human', 'sequence', 'virus', 'genetic']\n",
      "['dna', 'cell', 'human', 'sequenc', 'virus', 'genet']\n",
      "7\n",
      "['eye', 'human', 'brain', 'person', 'biology', 'body']\n",
      "['eye', 'human', 'brain', 'person', 'biolog', 'bodi']\n",
      "8\n",
      "['blood', 'cells', 'body', 'heart', 'system', 'human']\n",
      "['blood', 'cell', 'bodi', 'heart', 'system', 'human']\n",
      "9\n",
      "['dna', 'sequence', 'protein', 'molecules', 'genome', 'human']\n",
      "['dna', 'sequenc', 'protein', 'molecul', 'genom', 'human']\n",
      "10\n",
      "['organism', 'biology', 'body', 'species', 'system', 'human']\n",
      "['organ', 'biolog', 'bodi', 'speci', 'system', 'human']\n",
      "11\n",
      "['genetic', 'gene', 'human', 'dna', 'biology', 'species']\n",
      "['genet', 'gene', 'human', 'dna', 'biolog', 'speci']\n",
      "12\n",
      "['plant', 'species', 'animals', 'water', 'tree', 'growth']\n",
      "['plant', 'speci', 'anim', 'water', 'tree', 'growth']\n",
      "13\n",
      "['gene', 'dna', 'genetic', 'chromosome', 'protein', 'human']\n",
      "['gene', 'dna', 'genet', 'chromosom', 'protein', 'human']\n",
      "14\n",
      "['animals', 'species', 'plant', 'human', 'genetic', 'male']\n",
      "['anim', 'speci', 'plant', 'human', 'genet', 'male']\n",
      "15\n",
      "['biology', 'molecular', 'science', 'cells', 'field', 'human']\n",
      "['biolog', 'molecular', 'scienc', 'cell', 'field', 'human']\n",
      "16\n",
      "['enzyme', 'protein', 'dna', 'reaction', 'gene', 'cells']\n",
      "['enzym', 'protein', 'dna', 'reaction', 'gene', 'cell']\n",
      "17\n",
      "['data', 'gene', 'biology', 'sequence', 'human', 'database']\n",
      "['data', 'gene', 'biolog', 'sequenc', 'human', 'databas']\n",
      "18\n",
      "['protein', 'molecules', 'dna', 'gene', 'cells', 'sense']\n",
      "['protein', 'molecul', 'dna', 'gene', 'cell', 'sens']\n",
      "19\n",
      "['virus', 'cells', 'mutation', 'dna', 'protein', 'bacteria']\n",
      "['virus', 'cell', 'mutat', 'dna', 'protein', 'bacteria']\n",
      "20\n",
      "['cells', 'human', 'body', 'biology', 'chromosome', 'blood']\n",
      "['cell', 'human', 'bodi', 'biolog', 'chromosom', 'blood']\n",
      "21\n",
      "['hand', 'human', 'body', 'system', 'brain', 'potential']\n",
      "['hand', 'human', 'bodi', 'system', 'brain', 'potenti']\n",
      "22\n",
      "['amino', 'acid', 'protein', 'sequence', 'cells', 'human']\n",
      "['amino', 'acid', 'protein', 'sequenc', 'cell', 'human']\n",
      "23\n",
      "['experiment', 'muscle', 'physiological', 'blood', 'surface', 'person']\n",
      "['experi', 'muscl', 'physiolog', 'blood', 'surfac', 'person']\n",
      "24\n",
      "['sequence', 'database', 'genome', 'gene', 'biology', 'data']\n",
      "['sequenc', 'databas', 'genom', 'gene', 'biolog', 'data']\n",
      "25\n",
      "['insects', 'species', 'body', 'plant', 'natural', 'tree']\n",
      "['insect', 'speci', 'bodi', 'plant', 'natur', 'tree']\n",
      "26\n",
      "['cells', 'blood', 'gene', 'membrane', 'biology', 'tissue']\n",
      "['cell', 'blood', 'gene', 'membran', 'biolog', 'tissu']\n",
      "27\n",
      "['allele', 'genetic', 'mutation', 'model', 'gene', 'parent']\n",
      "['allel', 'genet', 'mutat', 'model', 'gene', 'parent']\n",
      "28\n",
      "['potential', 'membrane', 'cells', 'neurons', 'field', 'heart']\n",
      "['potenti', 'membran', 'cell', 'neuron', 'field', 'heart']\n",
      "29\n",
      "['system', 'biology', 'human', 'gene', 'body', 'protein']\n",
      "['system', 'biolog', 'human', 'gene', 'bodi', 'protein']\n",
      "30\n",
      "['body', 'cells', 'blood', 'animals', 'water', 'genetic']\n",
      "['bodi', 'cell', 'blood', 'anim', 'water', 'genet']\n",
      "31\n",
      "['bacteria', 'cells', 'bacterial', 'water', 'dna', 'mechanism']\n",
      "['bacteria', 'cell', 'bacteri', 'water', 'dna', 'mechan']\n",
      "32\n",
      "['water', 'plant', 'body', 'human', 'temperature', 'molecules']\n",
      "['water', 'plant', 'bodi', 'human', 'temperatur', 'molecul']\n",
      "33\n",
      "['disease', 'human', 'blood', 'data', 'genetic', 'immune']\n",
      "['diseas', 'human', 'blood', 'data', 'genet', 'immun']\n",
      "34\n",
      "['sequence', 'dna', 'protein', 'gene', 'rna', 'data']\n",
      "['sequenc', 'dna', 'protein', 'gene', 'rna', 'data']\n",
      "35\n",
      "['class', 'biology', 'cells', 'species', 'body', 'protein']\n",
      "['class', 'biolog', 'cell', 'speci', 'bodi', 'protein']\n",
      "36\n",
      "['birds', 'species', 'mammals', 'animals', 'tree', 'eye']\n",
      "['bird', 'speci', 'mammal', 'anim', 'tree', 'eye']\n",
      "37\n",
      "['skin', 'body', 'cells', 'blood', 'human', 'tissue']\n",
      "['skin', 'bodi', 'cell', 'blood', 'human', 'tissu']\n",
      "38\n",
      "['fitness', 'evolutionary', 'model', 'genetic', 'species', 'evolution']\n",
      "['fit', 'evolutionari', 'model', 'genet', 'speci', 'evolut']\n",
      "39\n",
      "['molecules', 'cells', 'molecular', 'water', 'energy', 'organism']\n",
      "['molecul', 'cell', 'molecular', 'water', 'energi', 'organ']\n",
      "40\n",
      "['tree', 'plant', 'species', 'region', 'evolutionary', 'water']\n",
      "['tree', 'plant', 'speci', 'region', 'evolutionari', 'water']\n",
      "41\n",
      "['offspring', 'parent', 'genetic', 'species', 'gene', 'male']\n",
      "['offspr', 'parent', 'genet', 'speci', 'gene', 'male']\n",
      "42\n",
      "['structure', 'protein', 'sequence', 'biology', 'cells', 'organism']\n",
      "['structur', 'protein', 'sequenc', 'biolog', 'cell', 'organ']\n",
      "43\n",
      "['rna', 'gene', 'protein', 'sequence', 'dna', 'structure']\n",
      "['rna', 'gene', 'protein', 'sequenc', 'dna', 'structur']\n",
      "44\n",
      "['physiological', 'mechanism', 'body', 'blood', 'biology', 'muscle']\n",
      "['physiolog', 'mechan', 'bodi', 'blood', 'biolog', 'muscl']\n",
      "45\n",
      "['neurons', 'brain', 'human', 'cells', 'potential', 'system']\n",
      "['neuron', 'brain', 'human', 'cell', 'potenti', 'system']\n",
      "46\n",
      "['food', 'human', 'digestion', 'body', 'animals', 'person']\n",
      "['food', 'human', 'digest', 'bodi', 'anim', 'person']\n",
      "47\n",
      "['physiological', 'human', 'sense', 'body', 'person', 'eye']\n",
      "['physiolog', 'human', 'sens', 'bodi', 'person', 'eye']\n",
      "48\n",
      "['chromosome', 'gene', 'human', 'cells', 'dna', 'female']\n",
      "['chromosom', 'gene', 'human', 'cell', 'dna', 'femal']\n",
      "49\n",
      "['body', 'human', 'cells', 'natural', 'organism', 'biology']\n",
      "['bodi', 'human', 'cell', 'natur', 'organ', 'biolog']\n",
      "50\n",
      "['die', 'human', 'species', 'organism', 'insects', 'animals']\n",
      "['die', 'human', 'speci', 'organ', 'insect', 'anim']\n",
      "51\n",
      "['brain', 'human', 'body', 'blood', 'system', 'cells']\n",
      "['brain', 'human', 'bodi', 'blood', 'system', 'cell']\n",
      "52\n",
      "['neurons', 'cells', 'potential', 'model', 'human', 'body']\n",
      "['neuron', 'cell', 'potenti', 'model', 'human', 'bodi']\n",
      "53\n",
      "['membrane', 'cells', 'protein', 'energy', 'molecules', 'acid']\n",
      "['membran', 'cell', 'protein', 'energi', 'molecul', 'acid']\n",
      "54\n",
      "['animals', 'human', 'species', 'biology', 'water', 'person']\n",
      "['anim', 'human', 'speci', 'biolog', 'water', 'person']\n",
      "55\n",
      "['growth', 'plant', 'bacteria', 'cells', 'mechanism', 'human']\n",
      "['growth', 'plant', 'bacteria', 'cell', 'mechan', 'human']\n",
      "56\n",
      "['experiment', 'biology', 'cells', 'water', 'gene', 'human']\n",
      "['experi', 'biolog', 'cell', 'water', 'gene', 'human']\n",
      "57\n",
      "['protein', 'structure', 'gene', 'sequence', 'human', 'biology']\n",
      "['protein', 'structur', 'gene', 'sequenc', 'human', 'biolog']\n",
      "58\n",
      "['tissue', 'cells', 'human', 'blood', 'body', 'mechanism']\n",
      "['tissu', 'cell', 'human', 'blood', 'bodi', 'mechan']\n",
      "59\n",
      "['mutation', 'gene', 'dna', 'cells', 'genetic', 'evolution']\n",
      "['mutat', 'gene', 'dna', 'cell', 'genet', 'evolut']\n",
      "60\n",
      "['energy', 'body', 'food', 'cells', 'brain', 'plant']\n",
      "['energi', 'bodi', 'food', 'cell', 'brain', 'plant']\n",
      "61\n",
      "['receptors', 'cells', 'neurons', 'system', 'brain', 'protein']\n",
      "['receptor', 'cell', 'neuron', 'system', 'brain', 'protein']\n",
      "62\n",
      "['organism', 'cells', 'dna', 'genetic', 'bacteria', 'biology']\n",
      "['organ', 'cell', 'dna', 'genet', 'bacteria', 'biolog']\n",
      "63\n",
      "['chemical', 'reaction', 'mechanism', 'structure', 'cells', 'brain']\n",
      "['chemic', 'reaction', 'mechan', 'structur', 'cell', 'brain']\n",
      "64\n",
      "['mammals', 'human', 'species', 'animals', 'evolutionary', 'evolve']\n",
      "['mammal', 'human', 'speci', 'anim', 'evolutionari', 'evolv']\n",
      "65\n",
      "['database', 'data', 'protein', 'genome', 'gene', 'species']\n",
      "['databas', 'data', 'protein', 'genom', 'gene', 'speci']\n",
      "66\n",
      "['person', 'brain', 'human', 'blood', 'biology', 'disease']\n",
      "['person', 'brain', 'human', 'blood', 'biolog', 'diseas']\n",
      "67\n",
      "['allele', 'gene', 'chromosome', 'parent', 'mutation', 'genetic']\n",
      "['allel', 'gene', 'chromosom', 'parent', 'mutat', 'genet']\n",
      "68\n",
      "['surface', 'water', 'cells', 'plant', 'sense', 'field']\n",
      "['surfac', 'water', 'cell', 'plant', 'sens', 'field']\n",
      "69\n",
      "['heart', 'blood', 'body', 'human', 'organism', 'muscle']\n",
      "['heart', 'blood', 'bodi', 'human', 'organ', 'muscl']\n",
      "70\n",
      "['oxygen', 'water', 'molecules', 'reaction', 'plant', 'animals']\n",
      "['oxygen', 'water', 'molecul', 'reaction', 'plant', 'anim']\n",
      "71\n",
      "['reaction', 'enzyme', 'cells', 'dna', 'system', 'mechanism']\n",
      "['reaction', 'enzym', 'cell', 'dna', 'system', 'mechan']\n",
      "72\n",
      "['genome', 'sequence', 'dna', 'data', 'gene', 'species']\n",
      "['genom', 'sequenc', 'dna', 'data', 'gene', 'speci']\n",
      "73\n",
      "['temperature', 'body', 'reaction', 'human', 'biology', 'sense']\n",
      "['temperatur', 'bodi', 'reaction', 'human', 'biolog', 'sens']\n",
      "74\n",
      "['body', 'biology', 'organism', 'eye', 'human', 'energy']\n",
      "['bodi', 'biolog', 'organ', 'eye', 'human', 'energi']\n",
      "75\n",
      "['region', 'sequence', 'gene', 'genome', 'dna', 'cells']\n",
      "['region', 'sequenc', 'gene', 'genom', 'dna', 'cell']\n",
      "76\n",
      "['scientific', 'human', 'plant', 'species', 'brain', 'biology']\n",
      "['scientif', 'human', 'plant', 'speci', 'brain', 'biolog']\n",
      "77\n",
      "['evolutionary', 'human', 'evolution', 'animals', 'species', 'biology']\n",
      "['evolutionari', 'human', 'evolut', 'anim', 'speci', 'biolog']\n",
      "78\n",
      "['mechanism', 'body', 'molecular', 'cells', 'biology', 'gene']\n",
      "['mechan', 'bodi', 'molecular', 'cell', 'biolog', 'gene']\n",
      "79\n",
      "['metabolic', 'body', 'human', 'reaction', 'cells', 'model']\n",
      "['metabol', 'bodi', 'human', 'reaction', 'cell', 'model']\n",
      "80\n",
      "['culture', 'cells', 'bacteria', 'coli', 'growth', 'system']\n",
      "['cultur', 'cell', 'bacteria', 'coli', 'growth', 'system']\n",
      "81\n",
      "['cancer', 'cells', 'mutation', 'growth', 'mechanism', 'gene']\n",
      "['cancer', 'cell', 'mutat', 'growth', 'mechan', 'gene']\n",
      "82\n",
      "['coli', 'protein', 'bacteria', 'gene', 'cells', 'culture']\n",
      "['coli', 'protein', 'bacteria', 'gene', 'cell', 'cultur']\n",
      "83\n",
      "['acid', 'cells', 'protein', 'plant', 'animals', 'water']\n",
      "['acid', 'cell', 'protein', 'plant', 'anim', 'water']\n",
      "84\n",
      "['drug', 'receptors', 'body', 'cells', 'blood', 'mechanism']\n",
      "['drug', 'receptor', 'bodi', 'cell', 'blood', 'mechan']\n",
      "85\n",
      "['potential', 'cells', 'neurons', 'system', 'biology', 'water']\n",
      "['potenti', 'cell', 'neuron', 'system', 'biolog', 'water']\n",
      "86\n",
      "['model', 'biology', 'protein', 'data', 'system', 'evolution']\n",
      "['model', 'biolog', 'protein', 'data', 'system', 'evolut']\n",
      "87\n",
      "['genome', 'dna', 'human', 'gene', 'data', 'species']\n",
      "['genom', 'dna', 'human', 'gene', 'data', 'speci']\n",
      "88\n",
      "['protein', 'cells', 'gene', 'dna', 'receptors', 'virus']\n",
      "['protein', 'cell', 'gene', 'dna', 'receptor', 'virus']\n",
      "89\n",
      "['muscle', 'body', 'cells', 'mechanism', 'human', 'tissue']\n",
      "['muscl', 'bodi', 'cell', 'mechan', 'human', 'tissu']\n",
      "90\n",
      "['rna', 'dna', 'genetic', 'structure', 'protein', 'natural']\n",
      "['rna', 'dna', 'genet', 'structur', 'protein', 'natur']\n",
      "91\n",
      "['natural', 'biology', 'human', 'system', 'mechanism', 'genetic']\n",
      "['natur', 'biolog', 'human', 'system', 'mechan', 'genet']\n",
      "92\n",
      "['cancer', 'cells', 'disease', 'mechanism', 'dna', 'body']\n",
      "['cancer', 'cell', 'diseas', 'mechan', 'dna', 'bodi']\n",
      "93\n",
      "['oxygen', 'blood', 'cells', 'body', 'plant', 'human']\n",
      "['oxygen', 'blood', 'cell', 'bodi', 'plant', 'human']\n",
      "94\n",
      "['male', 'female', 'human', 'gene', 'species', 'genetic']\n",
      "['male', 'femal', 'human', 'gene', 'speci', 'genet']\n",
      "95\n",
      "['animals', 'body', 'organism', 'biology', 'food', 'evolve']\n",
      "['anim', 'bodi', 'organ', 'biolog', 'food', 'evolv']\n",
      "96\n",
      "['evolution', 'natural', 'human', 'species', 'evolve', 'genetic']\n",
      "['evolut', 'natur', 'human', 'speci', 'evolv', 'genet']\n",
      "97\n",
      "['digestion', 'enzyme', 'food', 'system', 'dna', 'acid']\n",
      "['digest', 'enzym', 'food', 'system', 'dna', 'acid']\n",
      "98\n",
      "['evolve', 'evolution', 'human', 'evolutionary', 'organism', 'species']\n",
      "['evolv', 'evolut', 'human', 'evolutionari', 'organ', 'speci']\n",
      "99\n",
      "['bacterial', 'bacteria', 'cells', 'growth', 'disease', 'dna']\n",
      "['bacteri', 'bacteria', 'cell', 'growth', 'diseas', 'dna']\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(i)\n",
    "    print(top_words(i))\n",
    "    print(top_stemmed(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biology[\"pred\"] = biology[\"cluster\"].map(top_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biology.to_csv(\"pred1.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
